{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdkyum/.miniconda3/envs/engram/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import torch  # PyTorch의 경우\n",
    "torch.set_grad_enabled(False)\n",
    "print(torch.cuda.device_count())  # GPU 개수 확인\n",
    "print(torch.cuda.current_device())  # 현재 활성화된 GPU 확인\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [05:07<00:00, 102.66s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.23s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "model_pretrain = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "local_model_path = \"/home/kdkyum/workdir/llm_engram/model-output/Llama-2-13b-hf-bioS_multi5_permutes-lr5.0e-5/best_model\"\n",
    "\n",
    "model_finetune = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model_pretrain.eval()\n",
    "model_finetune.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor on device cuda:0 is not on the expected device meta!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, param \u001b[38;5;129;01min\u001b[39;00m state_dict_finetune\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict_pretrain:\n\u001b[0;32m---> 14\u001b[0m         diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_dict_pretrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m         all_diffs\u001b[38;5;241m.\u001b[39mappend(diff)\n\u001b[1;32m     16\u001b[0m         param_diffs[key] \u001b[38;5;241m=\u001b[39m diff\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_prims_common/wrappers.py:291\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, is_out\u001b[38;5;241m=\u001b[39m(out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_tensor\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tuple)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_names)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_prims_common/wrappers.py:143\u001b[0m, in \u001b[0;36melementwise_type_promotion_wrapper.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m promoted_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    137\u001b[0m     x: _maybe_convert_to_dtype(bound\u001b[38;5;241m.\u001b[39marguments[x], compute_dtype)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_promoting_arg_names  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    140\u001b[0m }\n\u001b[1;32m    141\u001b[0m bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mupdate(promoted_args)\n\u001b[0;32m--> 143\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Override the return_dtype if a dtype arg is present and not None\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m bound\u001b[38;5;241m.\u001b[39marguments:\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_refs/__init__.py:1788\u001b[0m, in \u001b[0;36msub\u001b[0;34m(a, b, alpha)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;66;03m# Carefully not to use prims.mul if b is a scalar / symint.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;66;03m# prims.mul always returns a tensor,\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;66;03m# which will mess with type promotion.\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m         b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m alpha\n\u001b[0;32m-> 1788\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle_noncontiguous_outputs([a, b], output)\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_ops.py:723\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_library/fake_impl.py:95\u001b[0m, in \u001b[0;36mconstruct_meta_kernel.<locals>.meta_kernel\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to run this operator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith meta Tensors (as opposed to FakeTensors), but this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.library.get_ctx(). Otherwise, please use FakeTensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_ctx_getter(error_on_ctx):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfake_impl_holder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_library/utils.py:31\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/library.py:1193\u001b[0m, in \u001b[0;36m_check_pystubs_once.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op\u001b[38;5;241m.\u001b[39m_defined_in_python:\n\u001b[1;32m   1192\u001b[0m     checked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m maybe_pystub \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_pystub(\n\u001b[1;32m   1196\u001b[0m     op\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mname, op\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39moverload_name\n\u001b[1;32m   1197\u001b[0m )\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maybe_pystub \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_library/custom_ops.py:592\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher.<locals>.fake_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.register_fake` to add an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake impl.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m     )\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abstract_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_prims/__init__.py:403\u001b[0m, in \u001b[0;36m_prim_elementwise_meta\u001b[0;34m(type_promotion, args_with_fixed_dtypes, *args)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args_with_fixed_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     args_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args_with_fixed_dtypes) \u001b[38;5;241m+\u001b[39m args_\n\u001b[0;32m--> 403\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_same_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_cpu_scalar_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_same_shape(\u001b[38;5;241m*\u001b[39margs_, allow_cpu_scalar_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    406\u001b[0m l2p_perm \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcompute_elementwise_output_logical_to_physical_perm(\u001b[38;5;241m*\u001b[39margs_)\n",
      "File \u001b[0;32m~/.miniconda3/envs/engram/lib/python3.10/site-packages/torch/_prims_common/__init__.py:764\u001b[0m, in \u001b[0;36mcheck_same_device\u001b[0;34m(allow_cpu_scalar_tensors, *args)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m arg\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m    757\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor on device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    759\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(arg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m         )\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type when checking for same device, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor on device cuda:0 is not on the expected device meta!"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the state dictionaries\n",
    "state_dict_finetune = model_finetune.state_dict()\n",
    "state_dict_pretrain = model_pretrain.state_dict()\n",
    "\n",
    "# Compute differences for all parameters (flattened) in the shared keys\n",
    "all_diffs = []\n",
    "param_diffs = {}\n",
    "for key, param in state_dict_finetune.items():\n",
    "    if key in state_dict_pretrain:\n",
    "        diff = torch.abs(param - state_dict_pretrain[key]).cpu().detach().flatten()\n",
    "        all_diffs.append(diff)\n",
    "        param_diffs[key] = diff\n",
    "\n",
    "all_diffs = torch.cat(all_diffs)\n",
    "threshold = torch.quantile(all_diffs, 0.99).item()\n",
    "print(\"Threshold for top 1% changes:\", threshold)\n",
    "\n",
    "# Aggregate counts of top-1% changes per layer and per submodule (MLP and Attention)\n",
    "layer_changes = {}  # { layer_id : {\"attn\": count, \"mlp\": count} }\n",
    "for key, diff in param_diffs.items():\n",
    "    # Only consider parameters that belong to a layer\n",
    "    # Expected key format: \"model.layers.{layer_id}.<submodule>....\"\n",
    "    if \"model.layers.\" in key:\n",
    "        parts = key.split(\".\")\n",
    "        layer_id = parts[2]\n",
    "        if layer_id not in layer_changes:\n",
    "            layer_changes[layer_id] = {\"attn\": 0, \"mlp\": 0}\n",
    "        # Classify whether the parameter is part of the MLP or Attention block.\n",
    "        if \"mlp\" in key:\n",
    "            layer_changes[layer_id][\"mlp\"] += (diff >= threshold).sum().item()\n",
    "        elif \"attn\" in key:\n",
    "            layer_changes[layer_id][\"attn\"] += (diff >= threshold).sum().item()\n",
    "\n",
    "# Sort layer indices numerically\n",
    "layers = sorted(layer_changes.keys(), key=lambda x: int(x))\n",
    "attn_counts = [layer_changes[layer][\"attn\"] for layer in layers]\n",
    "mlp_counts = [layer_changes[layer][\"mlp\"] for layer in layers]\n",
    "\n",
    "# Plot the count of top 1% changed parameters per layer for Attention and MLP modules.\n",
    "x = np.arange(len(layers))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - width/2, attn_counts, width, label='Attention')\n",
    "ax.bar(x + width/2, mlp_counts, width, label='MLP')\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Count of top 1% changed parameters')\n",
    "ax.set_title('Localization of Top 1% Most Changed Parameters per Layer')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(layers)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
